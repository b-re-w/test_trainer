{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a9da5cc",
   "metadata": {},
   "source": [
    "# MXFP4 Training Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42784c61",
   "metadata": {},
   "source": [
    "## Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Aug 24 05:54:12 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.94                 Driver Version: 560.94         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4050 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   52C    P8              1W /   78W |      31MiB /   6141MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      5356    C+G   ...1yjt4zspk6g\\SamsungSettingsHost.exe      N/A      |\n",
      "|    0   N/A  N/A      9144      C   ...al\\Programs\\LM Studio\\LM Studio.exe      N/A      |\n",
      "|    0   N/A  N/A     13588    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     13692    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A     23372    C+G   ...1yjt4zspk6g\\SamsungSettingsHost.exe      N/A      |\n",
      "|    0   N/A  N/A     24956    C+G   ...ekyb3d8bbwe\\Microsoft.CmdPal.UI.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c29bcff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set CUDA Device Number\n",
    "DEVICE_NUM = 0\n",
    "ADDITIONAL_GPU = 0\n",
    "\n",
    "from os import environ\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([f\"{i+DEVICE_NUM}\" for i in range(0, ADDITIONAL_GPU+1)])\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8054a7f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27fd18d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "\n",
    "import wandb\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "103fbd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Using device - cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    if ADDITIONAL_GPU:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda\")  # torch.device(f\"cuda:{DEVICE_NUM}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    DEVICE_NUM = -1\n",
    "\n",
    "print(f\"INFO: Using device - {device}\" + (f\":{DEVICE_NUM}\" if ADDITIONAL_GPU else \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63287143",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: brew (brew-research) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\BREW\\Desktop\\RetentionLabs\\test_trainer\\wandb\\run-20250824_055433-99luer81</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/brew-research/MXFP4_Example/runs/99luer81' target=\"_blank\">Qwen3_8B_From_Scratch</a></strong> to <a href='https://wandb.ai/brew-research/MXFP4_Example' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/brew-research/MXFP4_Example' target=\"_blank\">https://wandb.ai/brew-research/MXFP4_Example</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/brew-research/MXFP4_Example/runs/99luer81' target=\"_blank\">https://wandb.ai/brew-research/MXFP4_Example/runs/99luer81</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/brew-research/MXFP4_Example/runs/99luer81?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1ed3243f7a0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROJECT_NAME = \"MXFP4_Example\"\n",
    "RUN_NAME = \"Qwen3_8B_From_Scratch\"\n",
    "\n",
    "# WandB Initialization\n",
    "wandb.init(project=PROJECT_NAME, name=RUN_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98db3020",
   "metadata": {},
   "source": [
    "## Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b6a84cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"Trelis/tiny-shakespeare\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9766b94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 셰익스피어 데이터셋\n",
    "dataset = load_dataset(dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71244064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Text': \"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\nSecond Citizen:\\nWould you proceed especially against Caius Marcius?\\n\\nAll:\\nAgainst him first: he's a very dog to the commonalty.\\n\\nSecond Citizen:\\nConsider you what services he has done for his country?\\n\\nFirst Citizen:\\nVery well; and could be content to give him good\\nreport fort, but that he pays himself with being proud.\\n\\nSecond Citizen:\\nNay, but speak not maliciously.\\n\\nFirst Citizen:\\nI say unto you, what he hath done famously, he did\\nit to that end: though soft-conscienced men can be\\ncontent to say it was for his country he did it to\\nplease his mother and to be partly proud; which he\\nis, even till the altitude of his virtue.\\n\\nSecond Citizen:\\nWhat he cannot help in his nature, you account a\\nvice in him. You must in no way say he is covetous.\\n\\nFirst Citizen:\\nIf I must not, I need not be barren of accusations;\\nhe hath faults, with surplus, to tire in repetition.\\nWhat shouts are these? The other side o' the city\\nis risen: why stay we prating here? to the Capitol!\\n\\nAll:\\nCome, come.\\n\\nFirst Citizen:\\nSoft! who comes here?\\n\\nSecond Citizen:\\nWorthy Menenius Agrippa; one that hath always loved\\nthe people.\\n\\nFirst Citizen:\\nHe's one honest enough: would all the rest were so!\\n\\nMENENIUS:\\nWhat work's, my countrymen, in hand? where go you\\nWith bats and clubs? The matter? speak, I pray you.\\n\\nFirst Citizen:\\nOur business is not unknown to the senate; they have\\nhad inkling this fortnight what we intend to do,\\nwhich now we'll show 'em in deeds. They say poor\\nsuitors have strong breaths: they shall know we\\nhave strong arms too.\\n\\nMENENIUS:\\nWhy, masters, my good friends, mine honest neighbours,\\nWill you undo yourselves?\\n\\nFirst Citizen:\\nWe cannot, sir, we are undone already.\\n\\nMENENIUS:\\nI tell you, friends, most charitable care\\nHave the patricians of you. For your wants,\\nYour suffering in this dearth, you may as well\\nStrike at the heaven with your staves as lift them\\nAgainst the Roman state, whose course will on\\nThe way it takes, cracking ten thousand curbs\\nOf more strong link asunder than can ever\\nAppear in your impediment. For the dearth,\\nThe gods, not the patricians, make it, and\\nYour knees to them, not arms, must help.\"}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2c3fc00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Text'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73540e95",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ec1cabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen3ForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11c8e3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_model_id = \"Qwen/Qwen3-0.6B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd9ad96bf9d4efeac8d0b4154e6d00b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/49 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reference_tokenizer = AutoTokenizer.from_pretrained(reference_model_id, use_fast=True)\n",
    "tokenized_datasets = dataset.map(lambda data: reference_tokenizer(data[\"Text\"], truncation=True, padding=True), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9cc8d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f9236",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-trainer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
