{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a9da5cc",
   "metadata": {},
   "source": [
    "# MXFP4 Training Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42784c61",
   "metadata": {},
   "source": [
    "## Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c29bcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CUDA Device Number\n",
    "DEVICE_NUM = 0\n",
    "ADDITIONAL_GPU = 0\n",
    "\n",
    "from os import environ\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([f\"{i+DEVICE_NUM}\" for i in range(0, ADDITIONAL_GPU+1)])\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8054a7f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9c9618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply amazon patch\n",
    "import sys\n",
    "sys.path.insert(0, \"./mxfp4_llm/patch_override_scripts/te1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fd18d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "\n",
    "import mx\n",
    "import transformer_engine.pytorch as te\n",
    "\n",
    "import wandb\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103fbd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    if ADDITIONAL_GPU:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda\")  # torch.device(f\"cuda:{DEVICE_NUM}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    DEVICE_NUM = -1\n",
    "\n",
    "print(f\"INFO: Using device - {device}\" + (f\":{DEVICE_NUM}\" if ADDITIONAL_GPU else \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63287143",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"MXFP4_Example\"\n",
    "RUN_NAME = \"Qwen3_8B_From_Scratch\"\n",
    "\n",
    "# WandB Initialization\n",
    "wandb.init(project=PROJECT_NAME, name=RUN_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98db3020",
   "metadata": {},
   "source": [
    "## Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6a84cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"Trelis/tiny-shakespeare\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9766b94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 셰익스피어 데이터셋\n",
    "dataset = load_dataset(dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71244064",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3fc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73540e95",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec1cabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen3ForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c8e3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_model_id = \"Qwen/Qwen3-0.6B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_tokenizer = AutoTokenizer.from_pretrained(reference_model_id, use_fast=True)\n",
    "tokenized_datasets = dataset.map(lambda data: reference_tokenizer(data[\"Text\"], truncation=True, padding=True), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9cc8d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f9236",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-trainer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
